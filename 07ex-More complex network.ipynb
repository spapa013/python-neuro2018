{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complex networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to traing a significantly more complex network on the MNIST digit classification dataset and see how much improvement can be seen in the network's performance (on the test set).\n",
    "\n",
    "We will see that as you make the network more complex, it will take significantly longer to train the network. If you continue to run these networks on a CPU (as in regular computation), soon the training time becomes prohibitive!\n",
    "\n",
    "We will see how you can improve the situation by **training the network on a GPU**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch # import the PyTorch package\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision # import trochvision package\n",
    "from torchvision import transforms # get torchvision's transforms subpackage\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a composite transform that first converts images to tensors and then normalize the images\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # converts images into Tensors\n",
    "    transforms.Normalize([0.1307], [0.3081])\n",
    "])\n",
    "\n",
    "# apply the transforms at the time of dataset loading\n",
    "training_set = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "                                          transform=image_transform)\n",
    "test_set = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "                                          transform=image_transform)\n",
    "\n",
    "batch_size = 512\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decently complex network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a much more complex network (although still would be considered very simple from the field's standard) that uses operations like **convolution** and **drop outs**. (Covering these opartions is beyond the scope of this course but you can find tons of references on them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and train this network from scratch. Here I'm using values of learning rate and number of epochs that I have discovered to work well when training this network. These adjustable values that cannot trained by the gradient descent are often referred to as **hyperparameters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 2.349897\n",
      "Epoch 0 Loss: 0.642527\n",
      "Epoch 1 Loss: 0.688585\n",
      "Epoch 1 Loss: 0.403539\n",
      "Epoch 2 Loss: 0.318222\n",
      "Epoch 2 Loss: 0.248594\n",
      "Epoch 3 Loss: 0.264044\n",
      "Epoch 3 Loss: 0.240270\n",
      "Epoch 4 Loss: 0.311739\n",
      "Epoch 4 Loss: 0.199477\n",
      "Epoch 5 Loss: 0.211345\n",
      "Epoch 5 Loss: 0.176335\n",
      "Epoch 6 Loss: 0.203774\n",
      "Epoch 6 Loss: 0.176574\n",
      "Epoch 7 Loss: 0.173708\n",
      "Epoch 7 Loss: 0.140088\n",
      "Epoch 8 Loss: 0.175960\n",
      "Epoch 8 Loss: 0.172420\n",
      "Epoch 9 Loss: 0.152381\n",
      "Epoch 9 Loss: 0.138275\n",
      "Training completed in 154.54 seconds\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.03, momentum=0.5)\n",
    "\n",
    "start = time.time()\n",
    "for epoch_idx in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        # reset the gradient before the next gradient step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate the network output\n",
    "        output = net(data)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # perform back propagation to compute gradients with respect to parameters!\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a gradient descent step on the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # report the loss every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {} Loss: {:.6f}'.format(epoch_idx, loss.item()))\n",
    "            \n",
    "duration = time.time() - start\n",
    "print('Training completed in {:.2f} seconds'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0668, Accuracy: 58753/60000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.eval() # put network into evaluation model\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# prevents unnecessary gradient computation during test - can lead to time and memory saving\n",
    "with torch.no_grad(): \n",
    "    for data, target in test_loader:\n",
    "        output = net(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() \n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.max(1, keepdim=True)[1] \n",
    "        \n",
    "        # count number of times where max probability matches the label index\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "# divide the test loss by number of samples in the test set\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this network can perform much better than our earlier networks, but it takes significantly longer to train!\n",
    "\n",
    "For your refernce the best network performance on MNIST to date is 99.79% on the test set! You can find (a bit outdated - from 2016) classification scores on MNIST and many other popular benchmark datasets [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding up your training with a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a significant speed up by placing the network and data on GPUs and letting computation take place there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING** The following code will only work if you are on a machine with a properly configured GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 2.301060\n",
      "Epoch 0 Loss: 0.553308\n",
      "Epoch 1 Loss: 0.522070\n",
      "Epoch 1 Loss: 0.299691\n",
      "Epoch 2 Loss: 0.338035\n",
      "Epoch 2 Loss: 0.216872\n",
      "Epoch 3 Loss: 0.233847\n",
      "Epoch 3 Loss: 0.181265\n",
      "Epoch 4 Loss: 0.250003\n",
      "Epoch 4 Loss: 0.223492\n",
      "Epoch 5 Loss: 0.244404\n",
      "Epoch 5 Loss: 0.152630\n",
      "Epoch 6 Loss: 0.165739\n",
      "Epoch 6 Loss: 0.188301\n",
      "Epoch 7 Loss: 0.159576\n",
      "Epoch 7 Loss: 0.146979\n",
      "Epoch 8 Loss: 0.210386\n",
      "Epoch 8 Loss: 0.175097\n",
      "Epoch 9 Loss: 0.117553\n",
      "Epoch 9 Loss: 0.138345\n",
      "Training completed in 46.82 seconds\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.to('cuda') # place the network on GPU!\n",
    "\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.03, momentum=0.5)\n",
    "\n",
    "start = time.time()\n",
    "for epoch_idx in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        # reset the gradient before the next gradient step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # send each batch to GPU so it can be processed by the network that's also on the GPU\n",
    "        data, target = data.to('cuda'), target.to('cuda')\n",
    "\n",
    "        # evaluate the network output\n",
    "        output = net(data)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # perform back propagation to compute gradients with respect to parameters!\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a gradient descent step on the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # report the loss every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {} Loss: {:.6f}'.format(epoch_idx, loss.item()))\n",
    "            \n",
    "duration = time.time() - start\n",
    "print('Training completed in {:.2f} seconds'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also test the network on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0648, Accuracy: 58811/60000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.eval() # put network into evaluation model\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# prevents unnecessary gradient computation during test - can lead to time and memory saving\n",
    "with torch.no_grad(): \n",
    "    for data, target in test_loader:\n",
    "        # place batch onto GPU\n",
    "        data, target = data.to('cuda'), target.to('cuda')\n",
    "        \n",
    "        output = net(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() \n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.max(1, keepdim=True)[1] \n",
    "        \n",
    "        # count number of times where max probability matches the label index\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "# divide the test loss by number of samples in the test set\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
