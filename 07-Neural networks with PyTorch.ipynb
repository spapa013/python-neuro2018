{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building neural networks with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we are going to look at how we can use a neural network library called PyTorch to build, train and evaluate our own neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always we are going to start with importing the essential scientific Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we are also going to import the PyTorch package `torch` as well as the associated `torchvision` package that provides means of downloading and handling popular machine learning datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # import the PyTorch package\n",
    "import torchvision # import trochvision package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors - enhanced NumPy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with PyTorch, you have to work with it's specialized type of object called `Tensor`'s that is built to represent **tensors** of arbitrary dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.Tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might realize that the `Tensor` appears to be extremeley similar to NumPy's array which is also capable of representing high dimension arrays (e.g. tensors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, there is almost a one-to-one correspondance between NumPy array's and Tensor's features and functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why PyTorch Tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the striking similarity, you may be wondering why PyTorch couldn't just use NumPy array instead of going through great effort of providing yet another high dimension array library with almost identical functionalities. \n",
    "\n",
    "One of the most critical reason that Tensor is a distinct object from NumPy array is the fact that **Tensor's can be placed on GPU memory** and **take advantage of the immense acceleration in computation provided by parallelization of GPUs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will demonstrate the striking difference in computation speed achieved when computing with GPUs vs CPUs later in this session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to design a neural network to achieve a task, you must first get your hands on the data so that you can **train your network on it**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load a **training set** that we are going to use to train our network and a separate **test set** that we'll use to evaluate the performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use convenience methods in `torchvision.datasets` to download various popular machine learning benchmark images. Here we are going to download [**MNIST**](http://yann.lecun.com/exdb/mnist/) which is a collection of handwritten digits along with labels (i.e. what digit was drawn).\n",
    "\n",
    "The MNIST dataset consists of a total of 70,000 images, of which 60,000 are desginated as the **training set** and 10,000 are designated as the **test set**. This standardized separation allows everyone around the world to evaluate and compare their model's performances with each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.MNIST('./data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns Torchvision's special **dataset** object that can be used to represent **supervised datasets** consisting of both inputs (i.e. images) and targets (i.e. digit labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_set[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)\n",
    "plt.title('Digit: {}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 5, figsize=(6, 6))\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    image, label = train_set[i]\n",
    "    ax.imshow(image)\n",
    "    ax.set_title('Digit: {}'.format(label))\n",
    "    ax.axis('off')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the test set in an identical fashion, passing in `train=False` into `MNIST`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torchvision.datasets.MNIST('./data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_set[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)\n",
    "plt.title('Digit: {}'.format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even before you start feeding in your images into a neural network, it is very common to perform some data transformations - modifying images in some fixed manner that makes it easier to work with them.\n",
    "\n",
    "One of the most common image transformation is **normalization**, where you first compute mean and standard deviation across all images (typically in the training set). You then subtract the mean from each image and also divide each image by the standard deviation. If you did this, and recomputed the mean and standard deviation across all images, you will find that they now have **mean of 0** and **standard deviation of 1**, and thus they are said to be **normalized**.\n",
    "\n",
    "Normalization helps ensure input image intensities stay within some expected range, allowing the network to not have to worry about large variations in image values that is otherwise visually uninteresting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when you load images from Torchvision, they are provided as Pillow package's Image object. Pillow is one of Python's popular image processing package, and there images are represented by a dedicated Image object with a lot of methods implementing common image processing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in PyTorch, networks only understands PyTorch Tensors, and thus we must convert the images from Pillow Image into PyTorch Tensor before we can pass the image into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve these two *transformations* by making use of Torchvision's transformation operations. You combine multiple transformation operations together and pass it at the time of dataset loading. This returns a dataset that **applies these transformations** automatically on all images! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a transformation that will:\n",
    "1. convert images into PyTorch tensors\n",
    "2. normalize the images against the mean of 0.1307 and standard deviation of 0.3081."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms # get torchvision's transforms subpackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a composite transform that first converts images to tensors and then normalize the images\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # converts images into Tensors\n",
    "    transforms.Normalize([0.1307], [0.3081])\n",
    "])\n",
    "\n",
    "# apply the transforms at the time of dataset loading\n",
    "training_set = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "                                          transform=image_transform)\n",
    "test_set = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "                                          transform=image_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now any image you access through the dataset has the transformation already applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = training_set[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, you define a new neural network by defining a **new class that inherits from nn.Module** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(5, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.fc(x) # fully connected layer\n",
    "        z = F.relu(y) # activation function\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this better, let's take a quick review of classes and learn the new concept of **object inheritance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past session, we have taken a look at defining a **class** to represent a grouping of data and functions, where each **instance of a class** or **object** can be thought of as representing a concrete unit that has **properties** and **behavior** (or **methods**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # assign name\n",
    "        \n",
    "    def title(self):\n",
    "        return \"an ordinary person.\"\n",
    "    \n",
    "    def greeting(self):\n",
    "        print('Hello! My name is {}.'.format(self.name))\n",
    "        print(\"I'm {}\".format(self.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar = Person('Edgar')\n",
    "john = Person('John')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar.greeting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john.greeting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that both `edgar` and `john` are objects of type (class) `Person`. They both have properties called `name` that is unique to each, and have the common behaviors (methods) called `title` that returns a string describing the person, and `greeting` that prints out a greeting message introducing themselves. Note that `greeting` method calls the method `title` in creating the intro statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key of **Object-Oriented Programming (OOP)** is to group certain data (e.g. `name`) with behavior (e.g. `greeting`, `title`) that when put together can be used to represent a conceptual grouping that may correspond to some real world *objects*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialization via inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine that you want to define a new **class** of object called `Scientist` that has everything that a `Person`  has (e.g.`name`, `greeting`, and `title`), but has extra property called `topic` that specifies their research topic, and has a new behavior (i.e. *method*) called `research` that finds a significant result at p-value < 0.05. \n",
    "\n",
    "Without worring much about code duplication, you could implement it as such: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Scientist:\n",
    "    def __init__(self, name, topic):\n",
    "        self.name = name\n",
    "        self.topic = topic\n",
    "        \n",
    "    def title(self):\n",
    "        return \"an ordinary person.\"\n",
    "    \n",
    "    def greeting(self):\n",
    "        print('Hello! My name is {}.'.format(self.name))\n",
    "        print(\"I'm {}\".format(self.title()))\n",
    " \n",
    "    def research(self, silent=False):\n",
    "        print('Performing a research on the topic {}...'.format(self.topic))\n",
    "        pvalue = random.random() # randomly pick a value between [0, 1)\n",
    "        \n",
    "        if pvalue < 0.05:\n",
    "            if not silent:\n",
    "                print('Results statistically significant with p-value={:0.3f}!! Publish!!'.format(pvalue))\n",
    "            return True\n",
    "        else:\n",
    "            if not silent:\n",
    "                print('Results was not significant with p-value={:0.3f}... Continue working...'.format(pvalue))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar = Scientist(name='Edgar', topic='computational neuroscience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar.greeting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar.research()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice that there is a lot of repeated code between a `Person` and a `Scientist`. Both have a property called `name` and methods called `title` and `greeting`.\n",
    "\n",
    "After all, a Scientist **is a** Person, right?\n",
    "\n",
    "When one class can be thought of as a **specialization** of another class, you can save alot of typing and code duplication by using **class inheritance**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Scientist(Person):  # Scientist inherits from Person\n",
    "    def __init__(self, name, topic):\n",
    "        super().__init__(name) # call __init__ of Person with name\n",
    "        self.topic = topic\n",
    "\n",
    "    def research(self, silent=False):\n",
    "        print('Performing a research on the topic {}...'.format(self.topic))\n",
    "        pvalue = random.random() # randomly pick a value between [0, 1)\n",
    "        \n",
    "        if pvalue < 0.05:\n",
    "            if not silent:\n",
    "                print('Results statistically significant with p-value={:0.3f}!! Publish!!'.format(pvalue))\n",
    "            return True\n",
    "        else:\n",
    "            if not silent:\n",
    "                print('Results was not significant with p-value={:0.3f}... Continue working...'.format(pvalue))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moku = Scientist(name='Moku', topic='physics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar.greeting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar.research()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `Scientist` class no longer implements the `greeting` and `title` methods, yet you can still call them on the instance of `Scientist`. This is because these methods were **inherited** from `Person` class.\n",
    "\n",
    "Furthermore, we call something funny inside the `__init__` method: `super().__init__(name)`. As you may be able to guess, this calls the initializer of `Person` or the **super class**, passing in the value it expects (e.g. `name` of the person). This allows any complex configuration that `Person` might have done in its `__init__` to be reused. \n",
    "\n",
    "Also, you would refer to `Scientist` as a **subclass** of the `Person` (alternatively, `Person` is a *super class* of `Scientist`). We also say that there exits **is-a** relationship between `Scientist` and `Person` - `Scirntist` **is-a** `Person`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can **override** super class's implementation of a method to give a new, specialized behavior to an existing method! Here, let's **override** the implementation of the method `title`, in effect cusotmizing the introduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Scientist(Person):  # Scientist inherits from Person\n",
    "    def __init__(self, name, topic):\n",
    "        super().__init__(name)\n",
    "        self.topic = topic\n",
    "        \n",
    "    # overriding title method\n",
    "    def title(self):\n",
    "        return \"a researcher in {}\".format(self.topic)\n",
    "\n",
    "    def research(self, silent=False):\n",
    "        print('Performing a research on the topic {}...'.format(self.topic))\n",
    "        pvalue = random.random() # randomly pick a value between [0, 1)\n",
    "        \n",
    "        if pvalue < 0.05:\n",
    "            if not silent:\n",
    "                print('Results statistically significant with p-value={:0.3f}!! Publish!!'.format(pvalue))\n",
    "            return True\n",
    "        else:\n",
    "            if not silent:\n",
    "                print('Results was not significant with p-value={:0.3f}... Continue working...'.format(pvalue))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar = Scientist(name='Edgar', topic='computational neuroscience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar.greeting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we were able to modify the behaivor of an already existing method `greeting` by overriding the behavior of the another method `title`. This pattern in which you can **customize** behavior of an already existing method by overriding another method happens quite commonly. In fact, we will soon encounter them in implementing our neural network in PyTorch!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks inherit from *nn.Module*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now armed with knowlege of class inheritance, let's take another look at a typical definition of a neural network in PyTorch. \n",
    "\n",
    "In PyTorch, any network **is a** `nn.Module`. In other words, you define a new class that inherits from `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(5, 10) # creates a new fully connected layer Module\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.fc(x) # fully connected layer\n",
    "        z = F.relu(y) # activation function\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that `MyNetwork` *inherits from* `nn.Module`. This gives our class `MyNetwork` with a lot of properties and methods that is already part of `nn.Module`, and this is precisely what allows you to define a new neural network with very little code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network (a module) in PyTorch (which is a class), typically consists of one or more other **modules** that you instantiate and hold onto as *properties*. Here, we are defining a single `nn.Linear` module which corresponds to a *fully-connected* linear layer connecting from 5 input neurons into 10 output neurons. We **instantiate** this module and assign it to the object's property named `fc` (standing for **f**ully **c**onnected layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the `__init__`, we have not computed anything. We simply instantiated a module and assigned it to a property for *later use*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real use of a PyTorch module comes in when you **instantiate** the class - that is, you create an object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This action just created a new **instance** of the network, with it's own network weights that can be trained!\n",
    "\n",
    "A key feature of a module is that you can use it like a function - it accepts an input and returns an output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 5) # a simple vector of 5 elements - or 5 input values\n",
    "\n",
    "y = net(x) # you use a module instance like a function!\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The secret behind this is the `forward` method we defined in the `MyNetwork` class:\n",
    "\n",
    "```python\n",
    " def forward(self, x):\n",
    "    y = self.fc(x)\n",
    "    z = F.relu(y)\n",
    "    return z\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method, we accepted a parameter `x`, and we used the fully-connected linear layer module `self.fc` as a function with `x` as the input!\n",
    "\n",
    "It turns out that `nn.Linear` is yet another **subclass** of `nn.Module` (that is, `nn.Linear` is a `nn.Module`) and thus can take input and return outputs. Our particular `self.fc` was configured to take in input vector of size 5 and output vector of size 10.\n",
    "\n",
    "The `F.relu` is then an element-wise operation that clips any value less than 0 to 0, while keeping positive values as is. These functions are typically referred to as the **activation functions**. Finally, the `forward` function returns the output of `relu` and this becomes the output of the whole network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, `MyNetwork` implemented a **single full-connected layer neural network with ReLU activation function** - one of the simplest networks you can construct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building network to classify images into digits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have seen how to build a network by defining a new class that inherits from `nn.Module`, let's try to implement a network that takes in a $28 \\times 28$ pixels gray scale image of a digit in MNIST and classifies them into 1 of the 10 digits!\n",
    "\n",
    "The input will be one or more images of size $28 \\times 28$, and we are going to set **the output to be a vector of size 10** where each position indicates a *log probability* that the image belongs to the specific digit.\n",
    "\n",
    "We'll start with a simplest possible implementation where we **flatten out** the input image into a vector of size 28 * 28 = 784. This will be **fully connected neural network with no output nonlinearity** linking 784 input neurons into 10 output neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784) # flattens an image of form N x 1 x 28 x 28 -> N x 784\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1) # make sure that probabilities add up to one, and then take log\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the network and run an image through it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = test_set[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a log of class probabilities, so we can exponentiate this to get the actual probability over classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(net(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surely enough, they add up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(net(image)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perhaps take the index with the largest probability as the network's best guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.exp(net(image))\n",
    "torch.argmax(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the label is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage our network is not going to performing well at all. \n",
    "\n",
    "That's expected because our network is **randomly initialized**! In order to get a reasonable performance, we need to **train** the network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to **train our network** by minimizing a **loss function** (also known as the cost function) - a function that evaluates how *off* we are from the true target. Chosing a good loss function can influence how well your network trains and ultimately performs on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of **N-way classification** problem where the output is a vector of size *N*, it's quite common to treat the output as the log probability of N classes, and optimize the network by miniminzing the **negative log likelihood** loss. \n",
    "\n",
    "This is conceptually similar to adjusting the network weights (parameters) so that the correct class would have the higest probability among choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training a neural network, you would follow a procedure called **gradient descent** to adjust the values of the network weights such that the loss function becomes smaller.\n",
    "\n",
    "One of the biggest strenghts of frameworks like PyTorch lies in the fact **it can compute gradient of the loss with respect to all parameters in the network automatically** for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On gradient descent and back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the gradient of the loss with respect to the weights, neural network packages like PyTorch (and pretty much any other similar packages) make use of technique called **back propagation**. Refer to my slides for details on how backpropagation works to compute the gradients, and how the gradients can be used to adjust the parameters with the gradient descent. Interested readers are strongly encouraged to refer to wonderful online resources such as [Neural Network and Deep Learning](http://neuralnetworksanddeeplearning.com/) online text book by Michael Nielson for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on a minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a full-fledged optimization, you will typically evaluate the loss on **all training dataset** and try to **optimize the joint loss** (e.g. sum of all losses). However, this requires computing the loss on all images everytime you make a small modification to the parameters, and for complex neural networks, this computational cost can be extremely prohibitive.\n",
    "\n",
    "Hence, you would typically **estimate** the joint loss by evaluating the loss on a randomly selected subsets of the input-target pairs, or on **a minibatch** (or simply a batch) of data.\n",
    "\n",
    "Performing gradient descent on randomly sampled subsets of the training set is known as (minibatch) **stochastic gradient descent** (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to perform minibatch SGD, we need a way to construct a random minibatch from the training datasets. Fortunately, this is easy to achive using PyTorch's `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size) # by default shuffle is False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataLoader` is an iterable, that returns batches of input target pairs of specified *batch size*. We can take a look at what it will return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, t in training_loader:\n",
    "    print('Images:', x.shape)\n",
    "    print('Labels:', t.shape)\n",
    "    break # quit after one iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for both the images (inputs) and the labels (targets), the **first dimension is the batch dimension**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating through the minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to get a minibatch, let's start putting together a training framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNetwork()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(training_loader):\n",
    "    # evaluate the network output\n",
    "    output = net(data)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = F.nll_loss(output, target)\n",
    "    \n",
    "    # MISSING! Perform back propagation to compute gradient and perform gradient descent step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code successuflly steps through 60,000 training images in batch of 64, evaluate the network on the inputs, and computes the loss between the network prediction and the targets. \n",
    "\n",
    "However, we are still missing the step of computing the gradient of loss with respect to the parameters of the network, and performing gradient descent to actually adjust and therefore **train** the network parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve this by using:\n",
    "\n",
    "1. `backward` method call on the finall loss to trigger backpropagation computation, and \n",
    "2. An **optimizer** to adjust the values of the parameters based on the gradient - that is, perform a step of gradient descent\n",
    "\n",
    "Putting this all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNetwork()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(training_loader):\n",
    "    # reset the gradient before the next gradient step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # evaluate the network output\n",
    "    output = net(data)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = F.nll_loss(output, target)\n",
    "    \n",
    "    # perform back propagation to compute gradients with respect to parameters!\n",
    "    loss.backward()\n",
    "    \n",
    "    # perform a gradient descent step on the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final missing piece is some sort of **monitoring** by which we can observe that the network is actually training (that is, the loss decreases over training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # use time module to measure how long it takes to train a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNetwork()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "start = time.time()\n",
    "for batch_idx, (data, target) in enumerate(training_loader):\n",
    "    # reset the gradient before the next gradient step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # evaluate the network output\n",
    "    output = net(data)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = F.nll_loss(output, target)\n",
    "    \n",
    "    # perform back propagation to compute gradients with respect to parameters!\n",
    "    loss.backward()\n",
    "    \n",
    "    # perform a gradient descent step on the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # report the loss every 100 batches\n",
    "    if batch_idx % 100 == 0:\n",
    "        print('Loss: {:.6f}'.format(loss.item()))\n",
    "\n",
    "duration = time.time() - start\n",
    "print('Training completed in {:.2f} seconds'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that indeed our network appears to train over time as shown by the fact loss decreses over iterations. You can also see that loss eventually stop decreasing when the training reaches the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust the speed of the training by changing the value of the **learning rate**. Loosely speaking, learning rate controls the size of the step for each gradient descent step. \n",
    "\n",
    "Larger learning rate could lead to faster training but it can also easily stray youself away from an optimal solution. Achieving a good network training depends a lot on a good choice of the value of the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained network, it's time to evaluate it's performance on the test set. Training on one set and testing on a distinct set that was **not** used during the training is called **cross-validation**, and can be a good way to evaluate how well your network will **generalize** beyond the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval() # put network into evaluation model\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# prevents unnecessary gradient computation during test - can lead to time and memory saving\n",
    "with torch.no_grad(): \n",
    "    for data, target in test_loader:\n",
    "        output = net(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() \n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.max(1, keepdim=True)[1] \n",
    "        \n",
    "        # count number of times where max probability matches the label index\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "# divide the test loss by number of samples in the test set\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that our network actually performs ~84-90% correct on the digit classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the earlier example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = test_set[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.exp(net(image))\n",
    "torch.argmax(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a more complex network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we were already getting well above chance performance on digit classification with an extremely simple **single fully-connected layer network**. Now, let's try improving the result by training slight more complex network - **three layer fully-connected network with ReLU nonlinearity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784) # flattens an image of form N x 1 x 28 x 28 -> N x 784\n",
    "        x = F.relu(self.fc1(x)) # first fully connected layer followed by ReLU\n",
    "        x = F.relu(self.fc2(x)) # second fully connected layer followed by ReLU\n",
    "        x = self.fc3(x) # third fully connected layer *without* output ReLU\n",
    "        x = F.log_softmax(x, dim=1) # make sure that probabilities add up to one, and then take log\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not go ahead and train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ComplexNetwork()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for batch_idx, (data, target) in enumerate(training_loader):\n",
    "    # reset the gradient before the next gradient step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # evaluate the network output\n",
    "    output = net(data)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = F.nll_loss(output, target)\n",
    "    \n",
    "    # perform back propagation to compute gradients with respect to parameters!\n",
    "    loss.backward()\n",
    "    \n",
    "    # perform a gradient descent step on the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # report the loss every 100 batches\n",
    "    if batch_idx % 100 == 0:\n",
    "        print('Loss: {:.6f}'.format(loss.item()))\n",
    "        \n",
    "duration = time.time() - start\n",
    "print('Training completed in {:.2f} seconds'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice two things:\n",
    "1. It trains a bit slower\n",
    "2. The loss is still decreasing at the end of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is slower because the nework is a bit more complex and it requires more computations.\n",
    "\n",
    "The more critical is the second point. To deal with the fact that it is still decreasing, we should not limit ourselves to a single pass through our training set, but rather go through numerous pass through it. Each complete pass through the training set is called **an epoch**, so here we want to perform **multiple epoch** training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ComplexNetwork()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.005)\n",
    "\n",
    "start = time.time()\n",
    "for epoch_idx in range(5):\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        # reset the gradient before the next gradient step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate the network output\n",
    "        output = net(data)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # perform back propagation to compute gradients with respect to parameters!\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a gradient descent step on the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # report the loss every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {} Loss: {:.6f}'.format(epoch_idx, loss.item()))\n",
    "            \n",
    "duration = time.time() - start\n",
    "print('Training completed in {:.2f} seconds'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it takes even longer to train because we are going through the dataset multiple times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval() # put network into evaluation model\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# prevents unnecessary gradient computation during test - can lead to time and memory saving\n",
    "with torch.no_grad(): \n",
    "    for data, target in test_loader:\n",
    "        output = net(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() \n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.max(1, keepdim=True)[1] \n",
    "        \n",
    "        # count number of times where max probability matches the label index\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "# divide the test loss by number of samples in the test set\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But indeed, we see improvement in the network performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
